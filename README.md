# ✋Gestura: AI-Powered Sign Language Translation 🤖
🚀**Gestura** is an AI-powered real-time translator that converts hand gestures from sign language into **text** and **speech**, aiming to bridge communication gaps for the hearing and speech impaired.

Built using **Python**, **MediaPipe**, **OpenCV**, and a **Random Forest classifier**, this project brings machine learning and accessibility together in a meaningful way. .🧏‍♂️🔤

## 🌟Project Overview
Sign language is vital for millions worldwide, but the language barrier limits its understanding among non-signers. **Gestura** addresses this issue by using computer vision and machine learning to interpret hand gestures in real time and translate them into spoken or written language.
  
## 👨‍💻Features
 -✅Real-time sign recognition:Captures hand gestures using a webcam and processes them using MediaPipe’s hand landmark detection . 📹  
 -✅ Landmark Extraction & Gesture Analysis: Uses hand keypoints to identify specific gesture patterns .🌍 
 -✅ Machine learning translation: A **Random Forest model** classifies recognized gestures and maps them to corresponding text.🗣️ 
 -✅Text-to-Speech Output: Integrates speech synthesis to voice out the translated gesture  📹.

## ⚙️ Tech Stack

- **Languages:** Python  
- **Libraries/Tools:** MediaPipe, OpenCV, Scikit-learn, NumPy, pyttsx3 / gTTS  
- **Algorithm:** Random Forest Classifier  
- **Interface:** Webcam-based CLI

## 🌟Getting Started
  1.🔧 Installation:
  ```
   #Clone the repository
   git clone https://github.com/Shristirajpoot/Gestura.git
   
   #Navigate to the project directory
   cd sign-language-detector-flask-python
  ```
  
  2. **Install the required dependencies** using the following command:

  ```bash
    pip install -r requirements.txt
  ```
   
  3. Run the application:
  ```
   python sign-language-detector-flask-python.py
  ```
   
  3. Interact with the translator :
   - Activate the camera for real-time gesture recognition.
   - Perform a supported hand gesture.
   - Watch it translate to text or hear it via text-to-speech
## 🌟Screenshots🎨 
  ### Account Page
![Screenshot (50)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-28%20105641.png)

  ### Home Page
![Screenshot (104)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20221835.png)
### Login Page
![Screenshot (104)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20221905.png)

###  📸Dashboard Page
![Screenshot (104)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20221937.png)

### Feedback Page
![Screenshot (104)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20222018.png)
### Tables Page
![Screenshot (104)](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20222513.png)

  ### 🌙Camera Page
![hand-signs-of-the-ASL-Language.png](https://github.com/Shristirajpoot/Gestura/blob/main/Screenshot%202025-01-30%20224408.png)

## 🎥 Demo Video

[![Watch the demo](https://img.youtube.com/vi/sVI3OwGbkoI/0.jpg)](https://www.youtube.com/watch?v=sVI3OwGbkoI)

 ## 🛠️ What I Learned
-Implementing computer vision with MediaPipe
-Applying machine learning for gesture classification
-Building accessibility-focused real-time applications
-Understanding model training, preprocessing, and inference with live inputs
## 📜Project Report
🔓 For detailed insights, analysis, and findings, refer to the Project Report provided in the repository.
  
## 🤝Contributing
🙌 Contributions are welcome! If you'd like to contribute to this project, feel free to open issues, create pull requests, or reach out to discuss potential improvements.
